{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch로 Softmax Regression 구현"
      ],
      "metadata": {
        "id": "2usFVk-L5j9M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ_VLgPM5b5v",
        "outputId": "55b03f2b-27dc-4413-e401-1f3a415463d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 7.3163652420043945\n",
            "epoch: 100, cost: 0.34471017122268677\n",
            "epoch: 200, cost: 0.24629312753677368\n",
            "epoch: 300, cost: 0.18048618733882904\n",
            "epoch: 400, cost: 0.1371230036020279\n",
            "epoch: 500, cost: 0.10741613805294037\n",
            "epoch: 600, cost: 0.0862061008810997\n",
            "epoch: 700, cost: 0.07054899632930756\n",
            "epoch: 800, cost: 0.05868084356188774\n",
            "epoch: 900, cost: 0.04948682710528374\n",
            "epoch: 1000, cost: 0.042231157422065735\n",
            "epoch: 1100, cost: 0.03641216456890106\n",
            "epoch: 1200, cost: 0.0316784493625164\n",
            "epoch: 1300, cost: 0.0277786273509264\n",
            "epoch: 1400, cost: 0.024529235437512398\n",
            "epoch: 1500, cost: 0.021794205531477928\n",
            "epoch: 1600, cost: 0.019470909610390663\n",
            "epoch: 1700, cost: 0.017481112852692604\n",
            "epoch: 1800, cost: 0.01576409302651882\n",
            "epoch: 1900, cost: 0.014272335916757584\n",
            "epoch: 2000, cost: 0.012968292459845543\n",
            "epoch: 2100, cost: 0.011821798048913479\n",
            "epoch: 2200, cost: 0.010808685794472694\n",
            "epoch: 2300, cost: 0.009909093379974365\n",
            "epoch: 2400, cost: 0.009106887504458427\n",
            "epoch: 2500, cost: 0.00838859286159277\n",
            "epoch: 2600, cost: 0.007743018679320812\n",
            "epoch: 2700, cost: 0.007160826586186886\n",
            "epoch: 2800, cost: 0.006634160410612822\n",
            "epoch: 2900, cost: 0.006156271323561668\n",
            "epoch: 3000, cost: 0.005721406079828739\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x_train = torch.FloatTensor([[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], \n",
        "                             [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]])\n",
        "\n",
        "y_train = torch.FloatTensor([[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], \n",
        "                             [0, 1, 0], [1, 0, 0], [1, 0, 0] ]) # 찾으려는 클래스 개수 3개이군 !\n",
        "\n",
        "\n",
        " \n",
        "W = torch.randn(4, 3, requires_grad=True) #(row수, column 수)\n",
        "b = torch.randn(1, 3, requires_grad=True) # 클래스마다 b값이 존재해야하기 때문에 1, 3\n",
        "\n",
        "# print(W)\n",
        "# print(b)\n",
        "\n",
        "# 뭘 학습시킬지랑 lr 설정\n",
        "optim = torch.optim.Adam([W, b], lr=0.1)\n",
        "\n",
        "for epoch in range(3001): #3000번 학습\n",
        "\n",
        "  # 가설함수 dim-> 어떤 차원\n",
        "  h = torch.softmax(torch.mm(x_train, W) + b, dim=1)\n",
        "\n",
        "  # 각각의 값에 log 적용해서 y_train 곱하면 1인 애들만 나옴, 어떤 타입에 softmax를 적용할거냐 -> dim\n",
        "  cost = -torch.mean(torch.sum(y_train * torch.log(h), dim=1)) # cross entropy \n",
        "  # dim이 0이면 세로로 1이면 가로로 더한 것\n",
        "\n",
        "  # 위의 h, cost 아래처럼 사용가능\n",
        "  # h = (torch.mm(x_train, W)+b).softmax(dim=1)\n",
        "  # cost = -(y_train * torch.log(h)).sum(dim=1).mean()\n",
        "\n",
        "  optim.zero_grad() # 기울기 초기화\n",
        "  cost.backward()\n",
        "  optim.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    if epoch % 100 == 0 :\n",
        "      print(f\"epoch: {epoch}, cost: {cost.item()}\")\n",
        "\n",
        "  \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 새로운 테스트 값을 넣었을때 \n",
        "\n",
        "x_test = torch.tensor([[1,11,10,9], [1,3,4,3], [1,1,0,1]], dtype=torch.float)\n",
        "h_test = torch.softmax(torch.mm(x_test, W) + b, dim=1)\n",
        "\n",
        "print(h_test)\n",
        "\n",
        "# argmax 제일 큰 값 위치를 알려줌, dim을 1로 했으니 행렬마다의 제일 큰 값 위치를 리턴\n",
        "print(torch.argmax(h_test, dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxmAreER8Sc4",
        "outputId": "ecc03726-2e42-46cf-cf65-921740f8953c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000e+00, 2.2018e-17, 2.3150e-34],\n",
            "        [1.3654e-05, 7.4193e-01, 2.5805e-01],\n",
            "        [6.6801e-31, 1.5667e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([0, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 조금 더 깔끔한 Softmax"
      ],
      "metadata": {
        "id": "Dt74NHsO9Xbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F # cross_entropy를 사용하기 위해\n",
        "import torch.nn as nn\n",
        "\n",
        "x_train = torch.FloatTensor([[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], \n",
        "                             [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]])\n",
        "\n",
        "# [1,0,0], [0,1,0], [0,0,1] 대신 0, 1, 2를 쓰기\n",
        "y_train = torch.LongTensor([2, 2, 2, 1, 1 ,1 ,0, 0]) \n",
        " \n",
        "# W = torch.randn(4, 3, requires_grad=True) #(row수, column 수)\n",
        "# b = torch.randn(1, 3, requires_grad=True) # 클래스마다 b값이 존재해야하기 때문에 1, 3\n",
        "model = nn.Linear(4, 3) # W, b값을 랜덤하게 알아서 초기화 해줌\n",
        "# nn.Linear(입력벡터의 차원, 출력하려는 클래스 개수)\n",
        "\n",
        "\n",
        "# 뭘 학습시킬지랑 lr 설정\n",
        "# optim = torch.optim.Adam([W, b], lr=0.1)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(3001): #3000번 학습\n",
        "\n",
        "  # 가설함수\n",
        "  # h = torch.mm(x_train, W) + b\n",
        "  h = model(x_train)\n",
        "\n",
        "  # 각각의 값에 log 적용해서 y_train 곱하면 1인 애들만 나옴, 어떤 타입에 softmax를 적용할거냐 -> dim\n",
        "  # cost = -torch.mean(torch.sum(y_train * torch.log(h), dim=1)) # cross entropy \n",
        "  cost =  F.cross_entropy(h, y_train)\n",
        "  # F.cross_entropy는 softmax와 cross entropy를 합친 것\n",
        "\n",
        "  optim.zero_grad() # 기울기 초기화\n",
        "  cost.backward()\n",
        "  optim.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    if epoch % 100 == 0 :\n",
        "      print(f\"epoch: {epoch}, cost: {cost.item()}\")\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf0EhAjf8oRw",
        "outputId": "73324cae-a862-4a0e-887c-30b8f7360af9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 1.0027530193328857\n",
            "epoch: 100, cost: 0.23076365888118744\n",
            "epoch: 200, cost: 0.12131606042385101\n",
            "epoch: 300, cost: 0.07509488612413406\n",
            "epoch: 400, cost: 0.051318295300006866\n",
            "epoch: 500, cost: 0.03743790090084076\n",
            "epoch: 600, cost: 0.028604548424482346\n",
            "epoch: 700, cost: 0.022617556154727936\n",
            "epoch: 800, cost: 0.018360186368227005\n",
            "epoch: 900, cost: 0.015216359868645668\n",
            "epoch: 1000, cost: 0.012823299504816532\n",
            "epoch: 1100, cost: 0.010955934412777424\n",
            "epoch: 1200, cost: 0.00946823786944151\n",
            "epoch: 1300, cost: 0.00826212763786316\n",
            "epoch: 1400, cost: 0.007269493769854307\n",
            "epoch: 1500, cost: 0.006442024372518063\n",
            "epoch: 1600, cost: 0.005744324065744877\n",
            "epoch: 1700, cost: 0.005150290206074715\n",
            "epoch: 1800, cost: 0.004639991093426943\n",
            "epoch: 1900, cost: 0.00419826153665781\n",
            "epoch: 2000, cost: 0.003813134040683508\n",
            "epoch: 2100, cost: 0.0034753456711769104\n",
            "epoch: 2200, cost: 0.0031772726215422153\n",
            "epoch: 2300, cost: 0.002912986557930708\n",
            "epoch: 2400, cost: 0.002677504438906908\n",
            "epoch: 2500, cost: 0.0024668113328516483\n",
            "epoch: 2600, cost: 0.0022775051183998585\n",
            "epoch: 2700, cost: 0.0021068896166980267\n",
            "epoch: 2800, cost: 0.0019525161478668451\n",
            "epoch: 2900, cost: 0.0018124517519026995\n",
            "epoch: 3000, cost: 0.0016850128304213285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model) #bias b가 있냐 없냐\n",
        "for a in model.parameters():\n",
        "  print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLkAdb9AARGy",
        "outputId": "96ea570a-b816-4219-b062-81cccac66a1b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=4, out_features=3, bias=True)\n",
            "Parameter containing:\n",
            "tensor([[-23.1651,  -1.2998,  16.0987,  -4.6230],\n",
            "        [  2.6404,  -0.1235,  -0.4500,   1.9029],\n",
            "        [  5.0735,   0.6576,  -7.0886,   0.1015]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-29.2442,  -7.4250,  17.8522], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax Regression with Sklearn\n",
        " "
      ],
      "metadata": {
        "id": "rdXMCA1LAq0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearn에는 LogisticRegression에 Softmax regression이 함께 구현됨 \n",
        "# ⇒ y에 두 종류 이상의 값이 있을 경우 softmax regression 실행\n",
        " \n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "x_train = np.array([ [1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1, 7, 5, 5],\n",
        "                     [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7] ])\n",
        "\n",
        "# y에 0, 1, 2 등 둘 이상의 class가 존재 => softmax regression\n",
        "y_train = np.array([ 2, 2, 2, 1, 1, 1, 0, 0 ])\n",
        "\n",
        "model = LogisticRegression(penalty='none') # 모델 만들기\n",
        "model.fit(x_train, y_train) # 학습할거 넣어주고\n",
        "\n",
        "x_test = np.array([[1,11,10,9], [1,3,4,3], [1,1,0,1]]) # test case (값 예측)\n",
        "print(model.predict(x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmARyP0F_fBr",
        "outputId": "0680532b-fe81-4b5c-d64e-6b8416a52b9b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XYB6BNSaBKYo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}